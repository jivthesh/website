<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=icon href=https://rustav.org/img/logo.svg><link rel=manifest href=https://rustav.org/manifest.json><meta name=theme-color content=#ffffff><link rel=stylesheet href=https://rustav.org/css/bootstrap-reboot.css><link rel=stylesheet href=https://rustav.org/css/bootstrap.css><link rel=stylesheet href=https://rustav.org/css/font-awesome.min.css><link rel=stylesheet href=https://rustav.org/css/rust-av.css><title>Integration of AV-Metrics into rav1e, the AV1 Encoder - Rust AV</title></head><body><header class="navbar navbar-expand navbar-dark flex-column flex-md-row tk-navbar"><a class=navbar-brand href=https://rustav.org/><img src=https://rustav.org/img/logo.svg class=align-middle alt></a><div class="collapse navbar-collapse"><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://rustav.org/>Home</a></li><li class=nav-item><a class="nav-link  active" href=https://rustav.org/blog/2020-03-30-rav1e-and-av-metrics/>Blog</a></li><li class=nav-item><a class=nav-link href=https://rustav.org/community/>Community</a></li></ul></div><ul class="navbar-nav flex-row ml-md-auto d-none d-md-flex"><li class=nav-item><a class="nav-link p-2" href=https://github.com/rust-av target=_blank rel=noopener aria-label=GitHub><svg class="navbar-nav-svg" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 512 499.36" focusable="false"><title>GitHub</title><path d="M256 0C114.64.0.0 114.61.0 256c0 113.09 73.34 209 175.08 242.9 12.8 2.35 17.47-5.56 17.47-12.34.0-6.08-.22-22.18-.35-43.54-71.2 15.49-86.2-34.34-86.2-34.34-11.64-29.57-28.42-37.45-28.42-37.45-23.27-15.84 1.73-15.55 1.73-15.55 25.69 1.81 39.21 26.38 39.21 26.38 22.84 39.12 59.92 27.82 74.5 21.27 2.33-16.54 8.94-27.82 16.25-34.22-56.84-6.43-116.6-28.43-116.6-126.49.0-27.95 10-50.8 26.35-68.69-2.63-6.48-11.42-32.5 2.51-67.75.0.0 21.49-6.88 70.4 26.24a242.65 242.65.0 0 1 128.18.0c48.87-33.13 70.33-26.24 70.33-26.24 14 35.25 5.18 61.27 2.55 67.75 16.41 17.9 26.31 40.75 26.31 68.69.0 98.35-59.85 120-116.88 126.32 9.19 7.9 17.38 23.53 17.38 47.41.0 34.22-.31 61.83-.31 70.23.0 6.85 4.61 14.81 17.6 12.31C438.72 464.97 512 369.08 512 256.02 512 114.62 397.37.0 256 0z" fill="currentcolor" fill-rule="evenodd" /></svg></a></li></ul></header><div class="container-fluid tk-blog"><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 tk-sidebar"><div class="tk-docs-toggle d-md-none p-0 d-flex ml-3 collapsed align-item-center"><h1 class=tk-title>Integration of AV-Metrics into rav1e, the AV1 Encoder</h1><button class="btn btn-link" type=button data-toggle=collapse data-target=#tk-docs-nav aria-controls=tk-docs-nav aria-expanded=false aria-label="Toggle docs navigation"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 30 30" width="30" height="30" focusable="false"><title>Menu</title><path stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-miterlimit="10" d="M4 7h22M4 15h22M4 23h22"/></svg></button></div><nav class="tk-links collapse" id=tk-docs-nav><div class="tk-toc-item active"><p class=tk-toc-link>Blog Posts</p><ul class="nav tk-sidenav"><li><a href=https://rustav.org/blog/2020-03-30-rav1e-and-av-metrics/ class=active>Integration of AV-Metrics into rav1e, the AV1 Encoder</a></li><li><a href=https://rustav.org/blog/2019-12-19-rav1e-0.2.0/>rav1e 0.2.0 - Winter Solstice</a></li><li><a href=https://rustav.org/blog/2019-11-29-rav1e-and-gains-on-arm-devices/>rav1e and gains on ARM Devices</a></li><li><a href=https://rustav.org/blog/2019-11-20-rav1e-0.1.0/>rav1e 0.1.0 - Made in Tokyo</a></li><li><a href=https://rustav.org/blog/2019-04-09-using-rav1e-from-your-code/>Using rav1e - from your own code</a></li><li><a href=https://rustav.org/blog/2018-02-18-rust-av-and-multimedia/>Rust-av: Rust and Multimedia</a></li></ul></div></nav></div><div class="d-none d-xl-block col-xl-4 tk-toc"><div class=section-nav><nav id=TableOfContents><ul><li><ul><li><a href=#into-depths-of-analysis-and-its-tools>Into Depths of Analysis and its tools</a><ul><li><a href=#rtc-video-quality-tool><strong>RTC Video Quality Tool</strong></a></li><li><a href=#v-frame><strong>V-Frame</strong></a></li><li><a href=#av-metrics>AV-Metrics</a></li><li><a href=#integration-of-v-frame-in-av-metrics>Integration of v-frame in AV-Metrics</a><ul><li><a href=#using-v-frame-for-decoding>Using v-frame for decoding:</a></li></ul></li><li><a href=#integration-of-av-metrics-in-rav1e>Integration of AV-Metrics in rav1e</a></li></ul></li><li><a href=#how-it-looks-now>How it looks now!!</a></li><li><a href=#thanks>Thanks</a></li></ul></li></ul></nav></div></div><main class="col-12 col-md-9 col-xl-7 py-md-3 pl-md-3 tk-content tk-docs"><h1 class=tk-title>Integration of AV-Metrics into rav1e, the AV1 Encoder</h1><p class=tk-date>Vibhoothi</p><p>Hi,</p><p>Hope all are doing well, in this COVID-19 Outbreak and the lockdown, let us take a minute sometime to pray for the people who did pass away and who are under diagnosis with Corona,</p><p>So, this blog post aims to provide some technical overview of how <a href=https://github.com/xiph/rav1e/pull/2164>introduction of v-frame</a>, <a href=https://github.com/rust-av/av-metrics/pull/67>migration to v-frames for av-metrics</a> and also <a href=https://github.com/xiph/rav1e/pull/2184>integration of av-metrics into rav1e</a> will help for rapid analysis. Rapid analysis like measurement of encoderâ€™s complexity with metrics like PSNR, SSIM, A-PSNR, PSNR-HVS, MS-SSIM, CIEDE2000 etc.
These are based on <a href=https://tools.ietf.org/html/draft-ietf-netvc-testing-09>IETF NETVC testing draft</a>.</p><p>So if you are new to AV1 or rav1e or these things, In 2018, the Alliance for Open Media released its next-generation Video Codec AV1. AV1 is so far the most efficient royalty-free Video Codec Standard with respect to VP9, x264. Considering currently available AV1 Open-Source encoders libaom, SVT-AV1 and rav1e.</p><p>The Rust AV1 Encoder(rav1e) project by Mozilla, Xiph Organisation is a cleanroom AV1 Implementation having lower memory footprint making it a good starting point while others are either too slow or resource-intensive.</p><p>With the help of <a href=https://arewecompressedyet.com/>AreWeCompressedYet</a> runs and analysis on personal high-performance servers and machines, the post also aims to provide some minor analysis and other insights overtime.</p><p>Some notable things from the experimentation are as follows:</p><ul><li><p>rav1e is always having low complexity compared to SVT-AV1 and libaom since rav1e is having a lean implementation.</p><p><img src=https://i.imgur.com/tvPO11D.png alt=image650c>
<center><b>Figure 1</b>: General overview of rav1e File-structure</center><br></br></p></li><li><p>With the project&rsquo;s initial phase of support <a href=https://mindfreeze.videolan.me/blog/rav1e-and-gains-on-arm-devices>ARM(AArch64) Architecture</a>, which made rav1e more optimised with more than 35\% Percent improved Encoding Time and in Frames Per Second(FPS), which is a great boost for having rav1e on multiple platforms, which is first time happening in AV1.</p></li><li><p>Currently, rav1e is fastest AV1 Encoder on an ARM Devices when it was tested. The test was carried on Raspberry Pi 3 B+ which houses 1.4GHz Cortex-A53 with 1GB RAM as a baseline for CPU power so when running on powerful AArch64 devices like a mobile, it gives enhanced results.</p></li><li><p>Low-latency mode makes the encoding very fast compared to normal encoding at default speed levels, where the trade-off between speed and quality is being compromised. The time between the video frame entering the encoder and the packet containing it is the latency.</p></li><li><p>The optimisation is an iterative process. The below diagram explains the general approach of Analysis and optimisation workflow.</p><p><img src=https://i.imgur.com/xyfynLd.png alt=image650c>
<center><b>Figure 2</b>: Overall Optimisation Process</center></p></li><li><p>Usually, optimisation is done for a specific target like</p><ul><li>Speed, which targets Single execution time and latency.</li><li>Memory Usage, which targets Maximum resident set and allocation count.</li><li>Throughput, which is the number of results per unit of time and number of results per resource spend.</li><li>Quality, which is Application and use-case dependent.</li></ul><p>This analysis targets Speed and Memory Usage since those are the fundamental things which should make any encoder production-ready and impact will be very high.</p></li><li><p>There is no need to encode full videos which are having 1000 Frames to do benchmarking and per code-path analysis.</p><ul><li>One thing which is to be taken care is when the encoding preset is being tuned is, it should encode more than RDO Look Ahead like around 100 Frames for fair testing and good results of different types of the clip.</li><li><p>The Complexity of an encoder also depends on how large the functions and code blocks are there, see the below table for a quick comparison between rav1e and SVT-AV1.
<center></p><table><thead><tr><th>Language</th><th>files</th><th>blank</th><th>comment</th><th>code</th></tr></thead><tbody><tr><td>Assembly</td><td>44</td><td>4405</td><td>3371</td><td>71208</td></tr><tr><td>Rust</td><td>117</td><td>4494</td><td>5455</td><td>48370</td></tr><tr><td>YAML</td><td>6</td><td>49</td><td>3</td><td>669</td></tr><tr><td>Markdown</td><td>7</td><td>157</td><td>0</td><td>376</td></tr><tr><td>TOML</td><td>8</td><td>37</td><td>10</td><td>232</td></tr><tr><td>Bourne Shell</td><td>6</td><td>38</td><td>20</td><td>151</td></tr><tr><td>Python</td><td>3</td><td>39</td><td>17</td><td>144</td></tr><tr><td>Jupyter Notebook</td><td>1</td><td>0</td><td>478</td><td>129</td></tr><tr><td>SUM:</td><td>192</td><td>9219</td><td>9354</td><td>121279</td></tr></tbody></table><p></center><center><b>Table 1</b>: Lines of Code Distrubution of rav1e</center></p><p><center></p><table><thead><tr><th>Language</th><th>files</th><th>blank</th><th>comment</th><th>code</th></tr></thead><tbody><tr><td>Assembly</td><td>44</td><td>4419</td><td>3364</td><td>74276</td></tr><tr><td>C</td><td>81</td><td>2934</td><td>3180</td><td>30232</td></tr><tr><td>C/C++ Header</td><td>71</td><td>816</td><td>2406</td><td>3578</td></tr><tr><td>NAnt script</td><td>27</td><td>210</td><td>0</td><td>2506</td></tr><tr><td>YAML</td><td>2</td><td>46</td><td>0</td><td>486</td></tr><tr><td>JSON</td><td>1</td><td>0</td><td>0</td><td>427</td></tr><tr><td>Markdown</td><td>4</td><td>88</td><td>0</td><td>146</td></tr><tr><td>Python</td><td>1</td><td>12</td><td>28</td><td>35</td></tr><tr><td>SUM:</td><td>231</td><td>8506</td><td>8978</td><td>111686</td></tr></tbody></table><p></center><center><b>Table 2</b>: Lines of Code Distrubution of dav1d</center></p><p><center></p><table><thead><tr><th>Language</th><th>files</th><th>blank</th><th>comment</th><th>code</th></tr></thead><tbody><tr><td>C</td><td>245</td><td>29282</td><td>19765</td><td>229148</td></tr><tr><td>C/C++ Header</td><td>289</td><td>9646</td><td>16714</td><td>64523</td></tr><tr><td>C++</td><td>149</td><td>8223</td><td>11165</td><td>44228</td></tr><tr><td>make</td><td>25</td><td>3504</td><td>1925</td><td>7009</td></tr><tr><td>Markdown</td><td>20</td><td>1429</td><td>0</td><td>4438</td></tr><tr><td>Python</td><td>30</td><td>1396</td><td>2616</td><td>4436</td></tr><tr><td>Assembly</td><td>12</td><td>606</td><td>494</td><td>4291</td></tr><tr><td>CMake</td><td>100</td><td>579</td><td>657</td><td>3618</td></tr><tr><td>XML</td><td>14</td><td>0</td><td>0</td><td>1510</td></tr><tr><td>diff</td><td>2</td><td>4</td><td>102</td><td>1124</td></tr><tr><td>YAML</td><td>8</td><td>31</td><td>26</td><td>708</td></tr><tr><td>m4</td><td>3</td><td>52</td><td>60</td><td>393</td></tr><tr><td>Bourne Shell</td><td>3</td><td>35</td><td>86</td><td>382</td></tr><tr><td>DOS Batch</td><td>1</td><td>5</td><td>0</td><td>119</td></tr><tr><td>NAnt script</td><td>1</td><td>7</td><td>0</td><td>47</td></tr><tr><td>SUM:</td><td>902</td><td>54799</td><td>53610</td><td>365974</td></tr></tbody></table><p><center><b>Table 3</b>: Lines of Code Distrubution of SVT-AV1</center><br></br></center></p></li></ul><p>One thing which is to be noted is that SVT-AV1 is having both encoder and decoder while rav1e is encoder alone if the decoder is also added to rav1e, dav1d, AV1 Decoder also, still it will be having half of SVT-AV1 and <sup>1</sup>&frasl;<sub>3</sub> of libaom in terms of lines of code, which makes a good starting point.</p></li><li><p>The code-coverage is another important factor, having more than 50\% code-coverage is always good to have for any software, while rav1e is having around 77\% code-coverage.</p></li><li><p>For testing based on <a href=https://tools.ietf.org/html/draft-ietf-netvc-testing-09>IETF NETVC testing draft</a>, Objective-1 Tests are being focused primarily. The Videos should be YUV 4:2:0 Y4M Uncompressed Video which does not have Audio in it for better bitrate and faster encodes.</p></li><li><p>For having better analysis, the encoder should have a better dynamic analysis for faster research and development, for that, the following changes are being introduced,</p><ul><li>Introduction of rav1e and dav1d to RTC Video Quality tool for rapid analysis between encoders.</li><li>Introduction of new Library called v-frame for having Video configuration, definition and its missionary.</li><li>Migration of AV-Metrics, the Library which calculates video metrics like PSNR, SSIM, A-PSNR, SSIM, CIEDE2000, PSNR-HVS etc to use v-frame as standard configuration.</li><li>Integrate AV-Metrics to rav1e for rapid encoding analysis</li></ul><p>The above-mentioned things will be explained in the further sections.</p></li></ul><h2 id=into-depths-of-analysis-and-its-tools>Into Depths of Analysis and its tools</h2><h3 id=rtc-video-quality-tool><strong>RTC Video Quality Tool</strong></h3><p>RTC Video Quality Tool(<a href=https://github.com/vibhoothiiaanand/rtc-video-quality>rtc_tool</a>) is a project started by Google which is used for quick analysis between various encoders by generating graphs based on quality metrics.</p><p>The main use-case which the tool is targetting to cover is improved rapid analysis of rav1e with other industrial encoders like VP8, VP9, libaom-av1, OpenH264 which are being used to get the clear trade-offs and hotspots between encoders.</p><p>For rapid analysis of any encoder, an equally good or better decoder is required, in AV1 <a href=https://code.videolan.org/dav1d>dav1d</a>(<strong>Dav1d</strong> is a <strong>AV1</strong> <strong>D</strong>ecoder) is the best choice.</p><p><a href=https://arewecompressedyet.com/>AreWeCompressedYet</a> is always a better choice, the fundamental issue here is faster and rapid analysis, there would be a trade-off on more accurate results which is done on AWCY but for research and development purpose, this works better.</p><p><img src=https://i.imgur.com/5Rheycr.png alt=image650c>
<center><b>Figure 3</b>: Graphs generated using rtc_tool</center></p><p>The figures depict the variation of different encoders by producing more than 80 different graphs using this tool which also includes frame data with varying bitrates.</p><p>Command which is used to do encoding comparison between multiple encoders including rav1e:</p><pre><code class=language-bash>$ wget https://mf4.xiph.org/~ltrudeau/videos/nyan.y4m
$ ./generate_data.py --out=lib-rav1e.txt --encoders=aom-good:av1,
libvpx-rt:vp9,rav1e-default:rav1e nyan.y4m
$ python generate_graphs.py --out-dir rav1e_graphs  lib-rav1e.txt
</code></pre><h3 id=v-frame><strong>V-Frame</strong></h3><p><a href=https://crates.io/crates/v_frame>v-frame</a> is a library released by Team rav1e in March 2020, v-frame is library extracted from rav1e to have Video Configurations structures and it&rsquo;s missionary.
v-frame have moved the following to a separate library:</p><ul><li><p>Pixel: The pixel is located in <code>v_frame/src/pixel.rs</code> where there are several methods viz, Trait for casting between Primitive Types (<code>CastFromPrimitive</code>) like u8,u16, i16, i32. Types of Pixel that can be used, in v-frame is U8, used for 8-bit pixels, and U16 used for higher bitrate (HBDs) like 10 or 12 bits per pixel. The main trait of Pixel which is a type that can be used as a pixel type. v-frame&rsquo;s pixel is traits is similar to what the pixel is in rav1e.</p></li><li><p>Math: The Plane abstraction is using different maths functions like the floor, the ceiling of log2, aligning with the power of 2 and also then shifting values, finding Most Significant Bits, and rounding. These are just changed location form rav1e&rsquo;s util to <code>v_frame</code> (<code>src/util/math.rs</code> -&gt; <code>v_frame/src/math.rs</code>)</p></li><li><p>Plane: Plane in <code>v_frame</code> is again similar to what is being found in rav1e. In the Plane implementation wrap function is being modified or changed to <code>from_slice</code> where the input arguments are also changed to Address of an Array of Type Pixel(<code>&amp;[T]</code>) instead of Vector of type(<code>Vec&lt;T&gt;</code>) so a better representation of data which is easier refutable rather than an enclosing in a vector, and replacing the subsequent function of wrap with <code>from_slice</code>. For example:</p><p><code>Plane:::wrap(data, width)</code> -&gt;
<code>Plane::from_slice(&amp;data, width)</code></p></li><li><p>Chroma Sampling: Moving the ChromaSampling to v-frame so the Frame structure can use this. There are few things which are modified for cross-compatibility. Firstly, there is serialize feature which is being used by chromaSampling API which is using <a href=https://crates.io/crates/serde>serde</a> library with <code>derive</code> feature as a dependency. This is added as a feature among main Cargo definition as it is an optional dependency and sharing dependency is clean in rust.</p></li><li><p>Frame: Moving of Frame is not a straightforward method among the v-frame as it was having a lot of dependencies, function calls. The changes should be minimal so it will be easier to adapt an external Frame Structure in upstream.</p></li><li><p>Earlier padding constant was present inside frame implementation. Now, it is moved outside the implementation.</p><p><code>const LUMA_PADDING: usize = SB_SIZE + FRAME_MARGIN;</code> (<a href=https://github.com/xiph/rav1e/blob/master/src/frame/mod.rs#L22>L23</a>)</p></li><li><p>Luma padding is required for new frame allocation, while it is not available in the initialisation of new frame outside rav1e. Now it is required to initialise frames with padding constant as an additional argument for calculation of chroma padding.</p></li><li><p>In favour of minimal changes to upstream, a new trait FrameAlloc for Frame Structure is defined in upstream(rav1e), which defines new function without padding constant and returns the value.</p><pre><code class=language-rust>{
/// Public Trait Interface for Frame Allocation
 pub trait FrameAlloc {
   /// Initialise new frame default type
   fn new(width: usize, height: usize,
   chrop_sampling: ChromaSampling) -&gt; Self;
 }
 impl&lt;T: Pixel&gt; FrameAlloc for Frame&lt;T&gt; {
   /// Creates a new frame with the given parameters.
   /// new function calls new_with_padding function which takes
   ///  luma_padding as parameter
   fn new(
     width: usize, height: usize, chroma_sampling: ChromaSampling,
   ) -&gt; Self {
     v_frame::frame::Frame::new_with_padding(
       width, height, chroma_sampling, LUMA_PADDING )
   }
 }
</code></pre><p>For more details check <a href=https://github.com/xiph/rav1e/blob/master/src/frame/mod.rs#L46>src/frame/mod.rs</a></p></li></ul><p>Likewise, there are new simple trait functions for Calculating Padding (<a href=https://github.com/xiph/rav1e/blob/master/src/frame/mod.rs#L69>FramePad</a>), for the new tile of a frame(<a href=https://github.com/xiph/rav1e/blob/master/src/frame/mod.rs#L87>AsTile</a>) which has functions for tiles and mutable tiles(as_tile, as_tile_mut) for Frame because these functions are written inside Frame implementation. These are encoder specific ones which can be isolated from the v-frame.</p><h3 id=av-metrics>AV-Metrics</h3><p><a href=https://github.com/rust-av/av-metric>AV-Metrics</a> is a library introduced by Rust Audio-Video(<a href=https://github.com/rust-av>Rust-AV</a>) organisation which is targeted to measure the quality of compressed video comparing with uncompressed video and gives an output which consists of</p><ul><li><p>PSNR: Peak Signal to Noise Ratio or formally known as PSNR, is a traditional signal quality metrics measured in decibels. It is directly derived from mean square error(MSE) or its square root (RMSE). The formula being used to calculate is<br><code>20 * log10 { (MAX / RSE)}</code><br>or<br><code>10 * log10 ( MAX^2 / MSE )</code></p><p>This metric is being applied to both Luma(Y) and Chroma Planes (Cb/Cr). Also to be added that the error is computed over all the pixels in the video.</p></li><li><p>APSNR: Aligned PSNR is a metrics which is used to improve the accuracy of conventional PSNR. In APSNR there defines a dynamic window size as<br><code>w = sumFL + 1</code><br>Where,<br><code>w</code> = window size,<br><code>sumFL</code> = total of frame loss,<br>APSNR and PSNR are related but it is not possible to calculate full-video APSNR from whole-video PSNR, APSNR comes from a different method of averaging together the per-frame PSNRs.</p></li><li><p>PSNR-HVS: The Peak Signal-to-Noise Ratio for the Human Visual System (PSNR-HVS) metric performs a DCT(Discrete Cosine Transformation) of 8x8 blocks of the image, weights the coefficient and calculates the PSNR of those coefficients. In AV-Metrics, weights are taken by Daala Tools. The normalized inverse quantisation matrix for 8x8 DCT is not the JPEG based matrix and better correlation to Mean Observer Score(MOS) than PSNR.</p></li><li><p>SSIM: Structural Similarity Image Metrics(<a href=http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf>SSIM</a>) is a still image quality metric introduced which computes a score for each individual pixel, using a window of neighbouring pixels after which the scores are averaged to produce a global score for the entire image. Original Paper produces a score between 0 and 1. In other words, the measurement or prediction of image quality is based on an initial uncompressed or distortion-free image as a reference. SSIM is designed to improve over traditional methods such as PSNR, MSE. For making it more linear in accordance with BD-Rate computation for videos.<br><code>10 * log10(1-SSIM)</code></p></li><li><p>MSSIM: Multi-Scale SSIM(<a href=http://www.cns.nyu.edu/~zwang/files/papers/msssim.pdf>MSSIM</a> is a variant of SSIM computed over subsampled versions of an image. It is designed to be a more accurate metric than SSIM. The multi-Scale method is a convenient way to incorporate image details at different resolutions. Inputs are a reference and distorted image signals, the system iteratively applies a low-pass filter and downsamples the filtered image by a factor of 2.</p></li><li><p>CIEDE2000: <a href=http://dx.doi.org/10.1155/2012/273723>CIEDE200</a> is a metric based on CIEDE color distances. It generates a single score taking all three(Y, Cb, Cr) Chroma Planes. Like other metrics, CIEDE200 does not consider Structural similarity. Implementation of CIEDE2000 in AV-Metrics includes lookup tables, binomial series and multiple conversions of different colour metrics.</p></li></ul><p>These are very much required for Objective Video Quality Assessment. Objective Assessments are done in place of subjective Video Quality Assessment for easy and iterative experiments. Most of the above metrics apply to luma planes, and individually to planes in frames.</p><h3 id=integration-of-v-frame-in-av-metrics>Integration of v-frame in AV-Metrics</h3><p><a href=https://github.com/rust-av/av-metrics/pull/67>Migrating</a> to v-frame type in AV-Metrics is very much required because common definitions and structures so it can be integrated easily into encoders and will be faster.</p><ul><li><p>Introduction of v-frame as a crate: The v-frame is being added as a dependency in the Cargo.toml of the AV-Metrics.</p></li><li><p>Using v-frame&rsquo;s Plane instead of PlaneData: Earlier the FrameInfo Structure had planes of type PlaneData which is having a width, height and data of type Vec as elements. But all the data are present in the Plane struct itself which is found in v-frame. The PlaneConfig from Plane Structure for getting the video Width, Height will be used.</p></li><li><p>Improvising preliminary checks of input video: Earlier the PlaneData Implementation has checks for width, the height of both input videos to find any mismatch, now the EncoderConfig from v-frame can be used to validate both, thus giving strict check because PlaneConfig has more than width and height like Data stride, Allocated height in Pixels, Width, Height, Decimator along the X and Y axis, Number of padding pixels on right and bottom, and also X and y where the data starts. This is made plausible by having a trait PlaneCompare for Plane.</p></li><li><p>Addition of ChromaWeight Trait: The chroma weight function from chroma sampling implementation is introduced as a trait. The weight is defined as follows, these are nothing but the relative impact of the chroma planes compared to luma.</p><pre><code class=language-rust> ChromaSampling::Cs420 =&gt; 0.25
 ChromaSampling::Cs422 =&gt; 0.5
 ChromaSampling::Cs444 =&gt; 1.0
 ChromaSampling::Cs400 =&gt; 0.0
</code></pre></li><li><p>Using Pixel from v-frame</p></li><li><p>Having Stride for calculation: Currently, the assertion in the calculation of PSNR-HVS checks plane&rsquo;s length &ldquo;=&rdquo; with the product of width and height which is replaced with &ldquo;&gt;=&rdquo; product of stride value and height, so more strict check gives better results. During the initial tests of integration to encode, it was noticed that without this strict, encoder crashes and panics.</p><pre><code class=language-diff> - assert!(plane1.data.len() == width * height);
 + assert!(plane1.data.len() &gt;= stride * height);
</code></pre></li></ul><h4 id=using-v-frame-for-decoding>Using v-frame for decoding:</h4><ul><li><p>The new VideoDetails structure is used to store details of video like width, height, bit-depth, Chroma Sampling and position of the video, timebase of a video and also padding constant.</p></li><li><p>Having a Rational structure which helps to create new rational number, return as reciprocal and also return as a floating-point number and also being used by Time Base. Calling <code>get_video_details</code> function which is of VideoDetails Structs as return type will give Video Details.</p></li><li><p>The <code>read_video_frame</code> function which reds the next frame from input video now as VideoDetails as an additional argument. The Decoder binary needs to be refactored in such a way that the Frame, Plane are used from the v-frame.</p></li><li><p>The function which returns the chroma sampling which is being matched, now it is same only but the input is of the type <code>y4m::ColorSpace</code> and return type are <code>(ChromaSamplng, ChromaSamplingPosition)</code>.</p></li><li><p>The <code>get_video_details</code> function is being made for <code>y4</code>::Decoder&lt;&rsquo;_&lsquo;, R&gt;`.</p></li><li><p>Most of the values are being returned with the help of constructors and these values are being returned in this function.</p></li><li><p>Updating Converting Chroma Data: Currently, the chroma data function takes PlaneData and chroma position, and bit depth as input with return type as <code>PlaneData&lt;T&gt;</code>. The additional structure Plane Data is obsolete and now it is being directly used from Plane. The plane data is now a simple clone of Plane&rsquo;s Data while earlier it was more complex enclosed in a Vector and having the length of the data and returns Plane Structure which has both data and PlaneConfig&rsquo;s Clone.</p></li><li><p>Updating Converting Chroma Data: Currently, the chroma data function takes PlaneData and chroma position, and bit depth as input with return type as <code>PlaneData&lt;T&gt;</code>. The additional structure Plane Data is obsolete and now it is being directly used from Plane. The plane data is now a simple clone of Plane&rsquo;s Data while earlier it was more complex enclosed in a Vector and having the length of the data and returns Plane Structure which has both data and PlaneConfig&rsquo;s Clone.
<code>diff
- let mut output_data = vec![T::cast_from(0u8);plane_data.data.len()];
+ let mut output_data = plane_data.data.clone();</code>
The Algorithms is ported from daala-tools, the vertical chroma sample position must be realigned to get accurate results else it will not get, having precise chroma calculation in an encoder point of view does not matter much but for a library, it does matter.</p><p>In future, the input as y4m frame and having chroma width and bytes as parameters and returns newly constructed frame.</p><pre><code class=language-rust>  convert_chroma_data(frame.get_u_plane(), chroma_s_pos, bit_depth, chroma_width, bytes);
</code></pre></li><li><p>The function <code>read_video_frame</code> function reads Frames and returns is of the type FrameInfo enclosed in Result.
<code>rust
fn read_video_frame&lt;T: Pixel&gt;(&amp;mut self,cfg: &amp;VideoDetails) -&gt; Result&lt;FrameInfo&lt;T&gt;, ()&gt;</code>
Earlier the function was returning FrameInfo Structure, which is now obsolete and replaced by a new frame initialised inside the method. For U, V Planes, conversion of chroma data is also requried. The <code>read_frame</code> was having FrameInfo piped to frame which is not required because passing information directly by doing calculation inside map function is faster which also includes the calculation of planes from y4m decoder and converting of chroma data happens. In the end, these planes are returned adding this to Frame Info.</p></li></ul><p>Overall with the new y4m decode approach, decoding capability was improved and it is now more efficient.</p><h3 id=integration-of-av-metrics-in-rav1e>Integration of AV-Metrics in rav1e</h3><p>Once both v-frame and av-metrics are built with the same Frame, Plane, PlaneConfig, Pixel, ChromaSampling, <a href=https://github.com/xiph/rav1e/pull/2184>integration</a> of AV-Metrics into rav1e without touching rav1e API Internals will be better. For that the following things are being done:</p><ul><li><p>Introduction of Reference Frame for Packet API: For the calculation of any metrics or screen change, the source/input/<a href=https://github.com/xiph/rav1e/pull/2186>reference frame</a> is required which are taken from the FrameData(input) to packets which is of the type <code>Option&lt;Arc&lt;Frame&lt;T&gt;&gt;&gt;</code></p></li><li><p>Breaking API: The <code>show_psnr</code> from EncoderConfig structure and psnr from Packet Structure are not useful thus removing because these are obsolete and are not required for calculation of metrics.</p></li><li><p>Introduction of <code>--metrics</code>: The <a href=https://github.com/xiph/rav1e/blob/master/src/bin/common.rs>common.rs</a>(<code>src/bin/common.rs</code>) defines CLI Options are being written. Also, the calculation of PSNR is also covered easily with this, it is happening like this:</p><pre><code class=language-rust>let metrics_enabled = if matches.is_present(&quot;METRICS&quot;) {
     MetricsEnabled::All
} else if matches.is_present(&quot;PSNR&quot;) {
     MetricsEnabled::Psnr
} else {
     MetricsEnabled::None
};
</code></pre><p>Also to be noted that <code>metrics_enabled</code> is being added to CliOptions Structure.</p></li><li><p>Calculation of QualityMetrics: With the new QualityMetrics struct, which has PSNR, PSNR-HVS, SSIM, MS-SSIM, CIEDE2000, A-PSNR(In-Future), VMAF(In-Future).The <code>calculate_frame_metrics</code> method takes two frames(source and output), bit depth, chroma sampling, metrics enabled as inputs and has a return type of QualityMetrics where it returns after calculation.</p></li><li><p>Calculation of metrics happens when the packet is being received in the main <a href=https://github.com/xiph/rav1e/blob/master/src/bin/rav1e.rs>encode loop</a>(<code>src/bin/rav1e.rs</code>) based on the input and output frame from Packet.</p></li><li><p>In the stats.rs itself, functions which fetches metrics from QualityMetrics strut and displays it to users.</p><pre><code class=language-rust>let psnr_hvs = sum_metric(&amp;self.frame_info, |fi| fi.metrics.psnr_hvs.unwrap().avg);
</code></pre></li><li><p>In production, the encoder will be independent of av-metrics and do not need as a dependency, The <a href=https://github.com/xiph/rav1e/blob/master/src/lib.rs>rav1e library</a>(<code>src/lib.rs</code>) will be not modified and there will no metrics module which will inside the binary and will be handled using config(cfg) flags.</p><pre><code class=language-rust> #[cfg(feature = &quot;metrics&quot;)]
</code></pre></li></ul><h2 id=how-it-looks-now>How it looks now!!</h2><p><img src=https://i.imgur.com/489dqFB.png alt=image650c></p><p><center><b>Figure 4</b>:CLI Output</center></p><h2 id=thanks>Thanks</h2><p>I would like to thank to Luca Barbato(<a href=https://github.com/lu-zero>lu_zero</a>), Josh Holmer(<a href=https://github.com/shssoichiro>soichiro</a>), David Micheal Barr(<a href=http://ba.rr-dav.id.au>barrbrain</a>), Nathan Egge(<a href=https://developer.mozilla.com/communities/people/nathan-egge/>unlord</a>), Christopher Montgomery(<a href=https://people.xiph.org/~xiphmont/demo>xiphmont</a>) and also all others from Team rav1e for the continous help and debugging, reviewing of the whole research work.</p><p><br></br></p><p>Freely,<br>~mindfreeze</p></main></div></div><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js integrity=sha384-3ceskX3iaEnIogmQchP8opvBy3Mi7Ce34nWjpBIwVTHfGYWQS9jwHDVRnpKKHJg7 crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/tether/1.3.7/js/tether.min.js integrity=sha384-XTs3FgkjiBgo8qjEjBk0tGmf3wPrWtA6coPfQDfFEY8AnYJwjalXCiosYRBIBZX8 crossorigin=anonymous></script><script src=https://rustav.org/js/bootstrap.min.js></script><script src=https://rustav.org/js/highlight.js></script><script>$(function(){$("pre code").each(function(i,block){if(block.className.indexOf('language-rust')>=0){var new_content='';var lines=block.textContent.split('\n');for(var i=0;i<lines.length;i++){if(lines[i].indexOf('# ')==0||lines[i]=='#'){continue}
new_content+=lines[i].trimRight()+'\n';}
block.textContent=new_content.replace(/\n\n\n/g,"\n\n").trimRight();}
hljs.highlightBlock(block);});});</script><script>$(document).ready(function(){let data;fetch('https://api.github.com/orgs/rust-av/repos?type=all').then(function(res){return res.json();}).then(function(json){data=json;document.getElementById("repos").innerHTML=data.length;})});</script><script>$(document).ready(function(){let data;let stargazersCount=0;fetch('https://api.github.com/orgs/rust-av/repos?type=all').then(function(res){return res.json();}).then(function(json){data=json;data.map((data)=>{stargazersCount=stargazersCount+data.stargazers_count
document.getElementById("stars").innerHTML=stargazersCount})})})</script><script>$(document).ready(function(){let data;let forksCount=0;fetch('https://api.github.com/orgs/rust-av/repos?type=all').then(function(res){return res.json();}).then(function(json){data=json;data.map((data)=>{forksCount=forksCount+data.forks_count
document.getElementById("forks").innerHTML=forksCount})})})</script><script>$(document).ready(function(){let data;let openIssuesCount=0;fetch('https://api.github.com/orgs/rust-av/repos?type=all').then(function(res){return res.json();}).then(function(json){data=json;data.map((data)=>{openIssuesCount=openIssuesCount+data.open_issues_count
document.getElementById("openissues").innerHTML=openIssuesCount})})})</script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js></script><script type=text/javascript>docsearch({apiKey:'d7b5b785798fe748621bcaa8301a2201',indexName:'tokio',inputSelector:'#search-input',debug:false});</script></body></html>